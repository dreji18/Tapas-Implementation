{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table data extraction with Tapas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tapas was introduced in this research paper https://arxiv.org/abs/2004.02349"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either clone the repository(https://github.com/google-research/tapas) and start working or follow this notebook. \n",
    "In the Tapas Github there is already a notebook which details the implementation on Colab https://colab.research.google.com/github/google-research/tapas/blob/master/notebooks/sqa_predictions.ipynb#scrollTo=uI6zyIM20Kw4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This notebook is for running the model in local with a custom dataframe of interest. Its the same implementation detailed in the colab notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's install the code from PyPI https://pypi.org/project/tapas-table-parsing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tapas-table-parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf # make sure tensorflow version > 2.0, tensorflow~=2.2.0 will be ideal\n",
    "import os \n",
    "import shutil\n",
    "import csv\n",
    "import pandas as pd\n",
    "import IPython\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "from tapas.utils import tf_example_utils\n",
    "from tapas.protos import interaction_pb2\n",
    "from tapas.utils import number_annotation_utils\n",
    "from tapas.scripts import prediction_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before making the custom directories to store the files, make sure to specify the local directory of your interest\n",
    "\n",
    "os.chdir(r'XXX\\XXX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have downloaded and updated all required files to run this code in my repo, so download or clone my repo into your local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpoint for prediction\n",
    "\n",
    "Please note this is base sized model trained on SQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('results/sqa/tf_examples', exist_ok=True)\n",
    "os.makedirs('results/sqa/model', exist_ok=True)\n",
    "with open('results/sqa/model/checkpoint', 'w') as f:\n",
    "  f.write('model_checkpoint_path: \"model.ckpt-0\"')\n",
    "for suffix in ['.data-00000-of-00001', '.index', '.meta']:\n",
    "  shutil.copyfile(f'tapas_sqa_base/model.ckpt{suffix}', f'results/sqa/model/model.ckpt-0{suffix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "vocab_file = \"tapas_sqa_base/vocab.txt\"\n",
    "config = tf_example_utils.ClassifierConversionConfig(\n",
    "    vocab_file=vocab_file,\n",
    "    max_seq_length=max_seq_length,\n",
    "    max_column_id=max_seq_length,\n",
    "    max_row_id=max_seq_length,\n",
    "    strip_column_names=False,\n",
    "    add_aggregation_candidates=False,\n",
    ")\n",
    "converter = tf_example_utils.ToClassifierTensorflowExample(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please note I have removed some unwanted lines and altered the predict function to read data from pandas dataframe and predict the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_interactions_to_examples(tables_and_queries):\n",
    "  \"\"\"Calls Tapas converter to convert interaction to example.\"\"\"\n",
    "  for idx, (table, queries) in enumerate(tables_and_queries):\n",
    "    interaction = interaction_pb2.Interaction()\n",
    "    for position, query in enumerate(queries):\n",
    "      question = interaction.questions.add()\n",
    "      question.original_text = query\n",
    "      question.id = f\"{idx}-0_{position}\"\n",
    "    for header in table[0]:\n",
    "      interaction.table.columns.add().text = header\n",
    "    for line in table[1:]:\n",
    "      row = interaction.table.rows.add()\n",
    "      for cell in line:\n",
    "        row.cells.add().text = cell\n",
    "    number_annotation_utils.add_numeric_values(interaction)\n",
    "    for i in range(len(interaction.questions)):\n",
    "      try:\n",
    "        yield converter.convert(interaction, i)\n",
    "      except ValueError as e:\n",
    "        print(f\"Can't convert interaction: {interaction.id} error: {e}\")\n",
    "        \n",
    "def write_tf_example(filename, examples):\n",
    "  with tf.io.TFRecordWriter(filename) as writer:\n",
    "    for example in examples:\n",
    "      writer.write(example.SerializeToString())\n",
    "\n",
    "def predict(table_data, queries):\n",
    "\n",
    "  table_data = table_data.astype(str)\n",
    "  \n",
    "  table1 = [table_data.columns.tolist()]\n",
    "  table1.extend(table_data.to_numpy().tolist())\n",
    "\n",
    "  examples = convert_interactions_to_examples([(table1, queries)])\n",
    "  write_tf_example(\"results/sqa/tf_examples/test.tfrecord\", examples)\n",
    "  write_tf_example(\"results/sqa/tf_examples/random-split-1-dev.tfrecord\", [])\n",
    "  \n",
    "  results_path = \"results/sqa/model/test_sequence.tsv\"\n",
    "  all_coordinates = []\n",
    "  df = table_data\n",
    "\n",
    "  try:\n",
    "      with open(results_path) as csvfile:\n",
    "        reader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "          coordinates = prediction_utils.parse_coordinates(row[\"answer_coordinates\"])\n",
    "          all_coordinates.append(coordinates)\n",
    "          answers = ', '.join([table1[row + 1][col] for row, col in coordinates])\n",
    "          position = int(row['position'])\n",
    "          print(\">\", queries[position])\n",
    "          print(answers)\n",
    "  except:\n",
    "      print()\n",
    "  return all_coordinates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Now its time to load your data and ask questions, Please note the sequence length of each row entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('test_df.xlsx')\n",
    "\n",
    "result = predict(df, [\"what were the drivers names?\",\n",
    "      \"of these, which points did patrick carpentier and bruno junqueira score?\",\n",
    "      \"how many points for Paul Tracy\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
